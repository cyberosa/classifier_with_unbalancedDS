{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goal of the project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to identify users that are most likely to convert to paying customers. We would like to build a scoring mechanism to rate different users. A customer with a higher rating ought to have a high chance of conversion,\n",
    "while lower rated customers would be less likely to convert. We have a dataset containing historical\n",
    "information about customers along with a binary label that shows whether they converted or not. The goal is to use this data to score potential new paid customers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal of this document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To complete an initial analysis of the available data and perform an evaluation in order to suggest a machine learning solution for this project. Part of the python code used for this notebook will be added to a another python file so that we can run it as a separate python script."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assumptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sensitive data has been already anonymized so there are no further actions from our side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Potential approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is labeled so we could initially consider this project as a *supervised machine learning classification* problem. Thus, the idea would be to build a **prediction model** where we can tell if a lead is going to become a paid customer or not. The feasibility of such a model is totally dependent on the available data and how relevant is that data for the final classification of the lead.\n",
    "Let´s start with the exploration and analysis of the provided dataset in order to detect any patterns or insights that might be helpful before building any model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA Steps\n",
    "1. Exploration: to understand better the dataset.\n",
    "2. Dataset cleaning: to remove not relevant data for the classifier.\n",
    "    - Missing Data\n",
    "    - Outliers\n",
    "3. Data transformation: all features must be numerical. Check ranges.\n",
    "    - Categorical Data\n",
    "    - Scaling numerical data\n",
    "4. Correlation analysis: to check dependencies between features and remove redundant data.\n",
    "5. Saving the final dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will perform some basic exploration about the different features, outliers, categorical data, missing data, balanced information, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../ub_dataset.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first column of the dataset is not documented in the provided file, but after asking the origin we received the information that is an ordering index so we can delete it from our dataset since it is not relevant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.iloc[:,1:]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have some string fields, so we will perform a separate analysis on categorical data to check how many different unique values do we have per each one.\n",
    "The *signup_date* can give us an idea of the time range of the dataset. This is important if we are trying to find patterns in customers. It could happen that the patterns customers showed 5 years ago, are not valid anymore. Let´s perform some time analysis of the customers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min(df.signup_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(df.signup_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus the timeline of data is from 2008 to 2019, more than 10 years."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of customers per year\n",
    "We could analyze which years were most successful in converting customers according to the provided data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"signup_date\"] = pd.to_datetime(df[\"signup_date\"])\n",
    "df[\"year\"] = df[\"signup_date\"].dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leads_per_year = df.groupby([\"year\", \"label\"]).size().to_frame(name='count').reset_index()\n",
    "leads_per_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.scatterplot(data=leads_per_year, x=\"year\", y=\"count\", hue=\"label\", palette=\"Set2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no successful leads in data before 2014. This could be used to filter just recent data where we see that the volume increased and also conversions happened. Besides due to the imbalance between the volumes of each type one potential technique that could be used to compensate is downsampling, i.e. reducing the samples of the major class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of leads per month of the year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"month\"] = df[\"signup_date\"].dt.month\n",
    "leads_per_month = df.groupby([\"month\", \"label\"]).size().to_frame(name='count').reset_index()\n",
    "sns.scatterplot(data=leads_per_month, x=\"month\", y=\"count\", hue=\"label\", palette=\"Set2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=leads_per_month[leads_per_month[\"label\"]==1], x=\"month\", y=\"count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The summer months and specially September seem to be a good period of the year when more conversions happened. Winter months are the worst."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the numerical features we can analyze the distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are negative values for four fields. For the timezone this might be desired but for the other ones, the -1 value might mean \"Null\" or there is no information about this field."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning\n",
    "We will use year to remove the first period with a very low volume of data that could introduce noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df[\"year\"]>2013]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features that cannot be used to predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *fakeId* is an internal identifier for the customer, so it is unique number and it will be a new number for a new lead. It is impossible to make any prediction just using this information so we can remove it from the list of relevant features. \n",
    "The *signup-date* is the timestamp of the signing moment so it gives us an idea of when the lead registered in the system. We completed before some initial analysis per year and per month. The year was used to filter relevant data and downsampling a bit the major class. We would like to keep month of the year and it might show some patterns. \n",
    "There is place for optimizations regarding this initial/basic *seasonality analysis* to probe some correlation between successful campaigns and the period of the year where the campaign was launched. This guess would require some extra time and conversations with marketing teams. Thus we discard signup_date and year. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['fakeId', 'signup_date', 'year'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outliers "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feature *place_within_tenant* is showing a maximal value of 261 very far away from the rest of the values. This could be an error or not, so it is a clear outlier but we do not have enough confidence in removing it. Besides the feature *company_employes_qty* is also showing some outliers with 4 orders of magnitude away from the 75% percentile. We decided not to remove any outliers but we consider it is worthy to mention them and to investigate them just to detect possible errors on the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the first exploration we already found some missing values in the form of negative values, -1, for some columns. We would like to transform these values into *nan* values to get an idea of which columns do have a huge volume of missing information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_fix = ['continent', 'country_code', 'city', 'region_code']\n",
    "for col in columns_fix:\n",
    "    df[col].replace(-1, np.nan, inplace = True)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_missing = df.isnull().sum() * 100 / len(df)\n",
    "missing_value_df = pd.DataFrame({'column_name': df.columns,\n",
    "                                 'percent_missing': percent_missing})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have 17 columns\n",
    "missing_value_df.sort_values('percent_missing', ascending=False, inplace=True)\n",
    "missing_value_df.head(17)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After checking these percentages, we have three candidates with more than 50% of missing data, so these features are not going to bring much value as predictors for a prediction model. Thus we delete the top three in this list. \n",
    "An alternative approach, instead of deletion, could be to transform them into a *synthetic* boolean feature meaning True if we have info and False if we do not have information. Another approach is imputing the nan values with another new value, however, given the high percentage of missing data, we will be automatically creating a new dominant class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['company_industrygroups', 'category_sectors', 'company_employes_qty'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the rest of features we will impute the *nan* values with a replacement after analyzing the categorical data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = ['lead_source', 'campaign_name', 'group_source', 'group_landing', 'type', 'role']\n",
    "for cat in categorical_cols:\n",
    "    df[cat] = df[cat].astype('category')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let´s analyze one by one by identifying the number of categories per feature and the counts we have for each category."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### lead_source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of unique categories\n",
    "len(df.lead_source.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.lead_source.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### campaign name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df.campaign_name.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.campaign_name.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### group_source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df.group_source.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.group_source.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### group_landing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df.group_landing.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.group_landing.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df.type.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.type.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df.role.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.role.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to these results, the fields that might require some transformations/actions are:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. campaign_name with more than 300 different categories\n",
    "2. role with more than 100 categories plus some weird values in other languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.campaign_name.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO this is not changing the range of the original category code\n",
    "# TODO we would need to scale this feature too with MinMax Scaler\n",
    "# we need to add the new category first\n",
    "df.campaign_name = df.campaign_name.cat.add_categories([\"other\"])\n",
    "df = df.apply(lambda x: x.mask(x.map(x.value_counts())<100, \"other\") if x.name == \"campaign_name\"  else x)\n",
    "df.campaign_name.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We managed to reduce from 317 to 33 categories. Now we repeat the same with the role feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.role.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are grouping minority roles within minor category\n",
    "df.role = df.role.cat.add_categories([\"minor\"])\n",
    "df = df.apply(lambda x: x.mask(x.map(x.value_counts())<100, \"minor\") if x.name == \"role\"  else x)\n",
    "df.role.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Successfully reduced from 120 categories (119 plus Nan) to 33 (32 plus Nan). The next step with categorical data would be to generate some identifier for each value and have a new feature for each selected field. However it is time to do something with those Nans in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for type is going to be a new category \"unknown\"\n",
    "df.type = df.type.cat.add_categories([\"unknown\"])\n",
    "df.type = df.type.fillna(\"unknown\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for role is going to be the existing category \"unknown\"\n",
    "df.role = df.role.fillna(\"unknown\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for place_within_tenant is going to be 0\n",
    "df.place_within_tenant = df.place_within_tenant.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for continent, region_code, country_code and city the percentage is very low\n",
    "# we decided to replace it with a 0, anyway it is a 5% so it is not going to affect much the distribution\n",
    "for col in columns_fix:\n",
    "    df[col] = df[col].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for nondst_utc_offset we decided to use an extreme value 27, again it is a 5%\n",
    "df.nondst_utc_offset = df.nondst_utc_offset.fillna(27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for campaign_name, group_source, group_landing, lead_source is going to be a new category \"missing\"\n",
    "columns_miss = [\"campaign_name\", \"group_source\", \"group_landing\", \"lead_source\"]\n",
    "for col in columns_miss:\n",
    "    df[col] = df[col].cat.add_categories([\"missing\"])\n",
    "    df[col] = df[col].fillna(\"missing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are good to go with the next step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let´s paint some distributions of the numerical fields to check ranges furthermore and to identify where we might need to do some scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"darkgrid\", palette=\"rocket\")\n",
    "sns.distplot(df.place_within_tenant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df.continent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df.country_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df.city)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df.region_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df.nondst_utc_offset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *continent* and *place_within_tenant* fields have only very few possible values, while *city* has a clear huge range of valid values. Besides the distribution of *place_within_tenant* is very skewed because of the outlier we already saw previously. Many machine learning algorithms perform better or converge faster when features are on a relatively similar scale and/or close to normally distributed. \n",
    "We are going to apply **MinMax Scaler** as it preserves the shape of the original distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_scale = [\"place_within_tenant\", \"continent\", \"country_code\", \"city\", \"region_code\", \"nondst_utc_offset\"]\n",
    "for col in columns_scale:\n",
    "    # the min max scaler requires a vector\n",
    "    transformer = MinMaxScaler().fit(df[col].values.reshape(-1, 1)) # single feature\n",
    "    transformed_data = transformer.transform(df[col].values.reshape(-1, 1))\n",
    "    df[col+\"_mm\"] = transformed_data[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let´s check the new range of the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df.place_within_tenant_mm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df.continent_mm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df.country_code_mm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df.city_mm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df.region_code_mm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df.nondst_utc_offset_mm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical columns into Numerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df.columns:\n",
    "    if df[col].dtype.name == 'category':\n",
    "        df[col + \"_cat\"] = df[col].cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns.get_loc(\"label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecting all transformed new columns after the *label* column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sel_df = df.iloc[:,12:]\n",
    "sel_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sel_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (20,10)\n",
    "plt.tight_layout()\n",
    "\n",
    "def paint_correlation_matrix(data):\n",
    "    #Draw correlation mtx\n",
    "    k = data.count(axis=1)[0] \n",
    "    corrmat = data.corr()\n",
    "    cols = corrmat.nlargest(k, 'label')['label'].index\n",
    "    cm = np.corrcoef(data[cols].values.T)\n",
    "    sns.set(font_scale=1.25)\n",
    "    hm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\n",
    "    plt.show()\n",
    "    \n",
    "paint_correlation_matrix(sel_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is not a strong correlation with the output we want to predict, given the provided data. The only field having the \"highest\" value is *lead_source*. Besides there is a strong correlation between the *utc_offset* field and the region, continent, city and country code. Something that is not a surprise given that the timezone is totally dependent on the geographical place, and all those fields are connected with the region. Then we could delete the utc_offset field from relevant predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can identify also a strong correlation between the geographical fields (country, continent, city and region), again expected. Thus we could keep city and region_code, since they both have the lowest correlation, and discard continent and country_code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sel_df.drop(['nondst_utc_offset_mm', 'continent_mm', 'country_code_mm'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra test: Using the non-scaled features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Including the same correlation matrix but using the non-scaled features just to probe that correlation factors were not affected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df = df.drop([\"place_within_tenant_mm\", \"continent_mm\", \"country_code_mm\", \"city_mm\", \"region_code_mm\", \"nondst_utc_offset_mm\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paint_correlation_matrix(raw_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving final dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sel_df.to_pickle(\"training_df\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://realpython.com/python-pandas-tricks/#5-use-categorical-data-to-save-on-time-and-space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://jakevdp.github.io/PythonDataScienceHandbook/03.04-missing-values.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://machinelearningmastery.com/how-to-score-probability-predictions-in-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/scale-standardize-or-normalize-with-scikit-learn-6ccc7d176a02?gi=d46d94ecce1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://scikit-learn.org/stable/modules/preprocessing.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- From the EDA:  we could identify redundant features (many geographical fields), plus not relevant ones (due to the huge ammount of missing data. We might use this information in this second approach to filter/clean data in the same way.\n",
    "- From the initial model: we built a pure classifier using the selected features and corrected the imbalance using different techniques. The model that gave us the best results was the one using SMOTE as a resampling technique. We will use this model and the confusion matrix as a baseline to compare whatever is produced with this second approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()  # for plot styling\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import preprocessing\n",
    "from time import time\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from imblearn.under_sampling import RandomUnderSampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brainstorming about different clustering algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. KMeans works better with spherical distributions of data and we do not know the shape of the multidimensional space.\n",
    "2. DBSCAN getting the parameters right can be hard so we discard this method.\n",
    "3. If available time: try other promising clustering methods to check which one is performing better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why not just a partition by region? Do we have enough data per region? During the exploration we analyzed the distribution of the data by region, city, continent, etc. Most of them have a wide range of values, some with more than 300) which makes not feasible training such a huge number of models. Besides whenever we have a new lead or a region not seen before, there will be no model for unseen regions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In conclusion we cannot base the clustering in only one condition because if that information for a new lead is missing we will have no means of assign the new lead to a cluster, unless we assign it a random one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we just want to build a quick probe of concept and we have limited time for this exercise we will not investigate further this approach unless we discovered that it is giving us some improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KMeans Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea is to perform some clustering first of the data to help identifying different regions and maybe to shape better patterns inside each cluster. Once we have these clusters we will build a different RandomForestClassifier for each cluster and compare the average results with the previous baseline.\n",
    "Steps of this approach:\n",
    "1. Collect cleaned dataset: with the transformed and scaled features, removing non-relevant ones.\n",
    "2. K-Means: find the optimal number of cluster for this dataset. Prepare K-means with that configuration.\n",
    "3. For each cluster train a different classifier and get the scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reminder of the filters and transformations done in this dataset:\n",
    "- extract year and month from signup_date\n",
    "- remove fields that are not helping for the clustering such as *fakeId*, *signup_date* or *year* (but we will keep *month* as before) and fields with a high percentage of missing data such as *company_industrygroups*, *category_sectors*, *company_employes_qty*. \n",
    "- remove data from the initial years < 2014 (low volume and not a single successful lead info), risk of introducing noise.\n",
    "- transformation of categorical features generating the alternative ones with numbers (id of the category)\n",
    "- min-max scaler for numerical values.\n",
    "- remove redundant geographical features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"training_df\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 10 final features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find optimal number of clusters. K-means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let´s have a look at a couple of dimensions each time to get an idea how points are distributed in the space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context('poster')\n",
    "sns.set_color_codes()\n",
    "plot_kwds = {'alpha' : 0.25, 's' : 80, 'linewidths':0}\n",
    "plt.scatter(df.campaign_name_cat, df.lead_source_cat, c='b', **plot_kwds)\n",
    "frame = plt.gca()\n",
    "frame.axes.get_xaxis().set_visible(False)\n",
    "frame.axes.get_yaxis().set_visible(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(df.place_within_tenant_mm, df.city_mm, c='b', **plot_kwds)\n",
    "frame = plt.gca()\n",
    "frame.axes.get_xaxis().set_visible(False)\n",
    "frame.axes.get_yaxis().set_visible(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each k value, we will initialise k-means and use the inertia attribute to identify the sum of squared distances of samples to the nearest cluster centre.  **Inertia** is calculated as the sum of squared distance for each point to it's closest centroid, i.e., its assigned cluster. So $I=∑_i(d(i,cr))$ where cr is the centroid of the assigned cluster and d is the squared distance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let´s explore up to 15 clusters to check if we find an optimal value. We use \"k-means++\" instead of \"random\" to ensure centroids are initialized with some distance between them. In most cases, this will be an improvement over \"random\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def km_check(data, no_runs=25, no_clusters=15, k_means_method='k-means++'):\n",
    "    \"\"\"runs k-means clustering over no_clusters to get an idea of # clusters required for data.\n",
    "    returns a plot with clusters v SSE. \"\"\"\n",
    "    sse = []\n",
    "    for cluster in range(1, no_clusters):\n",
    "        kmeans = KMeans(n_jobs=-1,\n",
    "                        n_clusters=cluster,\n",
    "                        init=k_means_method,\n",
    "                        max_iter=500,\n",
    "                        n_init=no_runs)\n",
    "        kmeans.fit(data)\n",
    "        sse.append(kmeans.inertia_)\n",
    "    analysis = pd.DataFrame({'Cluster': range(1, no_clusters), 'SSE': sse})\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(analysis['Cluster'], analysis['SSE'], marker='o')\n",
    "    plt.xlabel('Number of clusters')\n",
    "    plt.ylabel('Inertia/SSE')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start = time()\n",
    "km_check(df)\n",
    "end = time()\n",
    "result = end - start\n",
    "print('Training time = %.3f seconds' % result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ideal number of clusters seems to be 4. Let´s try to paint some clusters for the previous graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_algorithm(data, algorithm, args, kwds):\n",
    "    \"\"\"fit an algorithm to the data\"\"\"\n",
    "    start = time()\n",
    "    labels = algorithm(*args, **kwds).fit_predict(data)\n",
    "    end = time()\n",
    "    result = end - start\n",
    "    print('Fitting time = %.3f seconds' % result)\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_clusters(algorithm, labels, feature1, feature2):\n",
    "    palette = sns.color_palette('viridis', np.unique(labels).max() + 1)\n",
    "    colors = [palette[x] if x >= 0 else (0.0, 0.0, 0.0) for x in labels]\n",
    "    plt.scatter(feature1, feature2, c=colors, **plot_kwds)\n",
    "    frame = plt.gca()\n",
    "    frame.axes.get_xaxis().set_visible(False)\n",
    "    frame.axes.get_yaxis().set_visible(False)\n",
    "    plt.title('Clusters found by {}'.format(str(algorithm.__name__)), fontsize=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = fit_algorithm(df, KMeans, (), {'n_jobs':-1,'n_clusters':4, 'n_init':25, 'init':'k-means++', 'max_iter':500,'random_state':42})\n",
    "plot_clusters(KMeans, labels, df.place_within_tenant_mm, df.city_mm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_clusters(KMeans, labels, df.campaign_name_cat, df.lead_source_cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are we happy with these clusters? It is difficult to say given that this is only a slice of the multi-dimensional space considered in the algorithm. However for the second slice we were able to catch a clear division between two clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a classifier for each cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to add a new column to the dataset with the assigned cluster to each point. This way we will split the data by each cluster and train a different model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_cluster_id(data, labels):\n",
    "    data['cluster_id'] = labels\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clustered = add_cluster_id(df,labels)\n",
    "df_clustered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_undersampling(data):\n",
    "    t = df.label.value_counts()\n",
    "    percentage = (t[1] * 100 / t[0])* 100\n",
    "    # if the percentage is less than 30% we use undersampling\n",
    "    return percentage < 30.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model(model, X0_train, y0_train):\n",
    "    start = time()\n",
    "    model.fit(X0_train,y0_train)\n",
    "    end = time()\n",
    "    result = end - start\n",
    "    return model\n",
    "    print('Training time = %.3f seconds' % result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test_sets(data):\n",
    "    # remove cluster_id\n",
    "    d = data.drop(['cluster_id'], axis=1)\n",
    "    X = d.drop(['label'], axis=1)\n",
    "    y = d['label']  # Labels\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3) # 70% training and 30% test\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_eval_model(data):\n",
    "    X_train, X_test, y_train, y_test = get_train_test_sets(data)\n",
    "    # similar configuration as the baseline model\n",
    "    clf=RandomForestClassifier(n_estimators= 200,\n",
    "         min_samples_split= 5,\n",
    "         min_samples_leaf= 4,\n",
    "         max_features= 'auto',\n",
    "         max_depth= 10,\n",
    "         bootstrap= False,\n",
    "         n_jobs=4)\n",
    "    start = time()\n",
    "    if check_undersampling(data):\n",
    "        undersample = RandomUnderSampler(sampling_strategy='majority')\n",
    "        X_train_under, y_train_under = undersample.fit_resample(X_train, y_train)\n",
    "        clf.fit(X_train_under,y_train_under)\n",
    "    else:\n",
    "        clf.fit(X_train,y_train)\n",
    "    end = time()\n",
    "    result = end - start\n",
    "    print('Training time = %.3f seconds' % result)\n",
    "    # get predictions\n",
    "    y_pred = clf.predict(X_test)\n",
    "    # get precision and recall \n",
    "    p = precision_score(y_pred, y_test, average=None)\n",
    "    print(\"precision scores\")\n",
    "    print(p)\n",
    "    r = recall_score(y_pred, y_test, average=None)\n",
    "    print(\"recall scores\")\n",
    "    print(r)\n",
    "    return p[0], r[0], p[1], r[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(\n",
    "    columns=['cluster_id', 'precision_conv', 'recall_conv', 'precision_non_conv', 'recall_non_conv'])\n",
    "for i in range(4):\n",
    "    row = dict()\n",
    "    row['cluster_id'] = i\n",
    "    # get the cluster dataframe\n",
    "    cluster_df = df_clustered[df_clustered['cluster_id'] == i]\n",
    "    print(\"Start training and evaluation for cluster {}\".format(i))\n",
    "    # train the model and evaluate it\n",
    "    p_0, r_0, p_1, r_1 = train_eval_model(cluster_df)\n",
    "    row['precision_conv'] = p_1\n",
    "    row['recall_conv'] = r_1\n",
    "    row['precision_non_conv'] = p_0\n",
    "    row['recall_non_conv'] = r_0 \n",
    "    # save results in a dataframe\n",
    "    results = results.append(row, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are not bad at all. Maybe the cluster 0 is the one that is not performing the best. So the average metrics for this solution would be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(4):\n",
    "    # get the cluster dataframe\n",
    "    cluster_df = df_clustered[df_clustered['cluster_id'] == i]\n",
    "    print(\"Size of cluster_{} = {}\".format(i,len(cluster_df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By clustering and running small models we did not see much improvement in the detection of converted leads. It is a nice way to probe that this approach is also valid and could potentially be considered if the data source becomes really big and no big-data resources are available. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://hdbscan.readthedocs.io/en/latest/comparing_clustering_algorithms.html\n",
    "\n",
    "https://machinelearningmastery.com/clustering-algorithms-with-python/\n",
    "\n",
    "https://realpython.com/k-means-clustering-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
